# 更新日志

## [1.3.0] - 2025-11-20

### 🔥 重大更新：阶段二每次发言的完整调试信息

#### 核心功能
现在可以查看每个评委**每次发言**时的完整上下文和原始响应！

**新增功能：**
1. **Context History（上下文历史）**
   - 记录每个评委发言时看到的所有历史消息
   - 包括初始消息和之前所有评委的发言
   - 按时间顺序展示，完整重现发言上下文

2. **Raw Response（原始响应）**
   - 保存每次发言的原始 AI 输出
   - 支持一键复制
   - 便于分析模型行为

3. **Model Name（模型标识）**
   - 显示每条消息使用的具体模型
   - 帮助诊断模型混淆问题

4. **系统配置检查页面**
   - 新增 `/config.html` 页面
   - 实时查看所有评委的模型映射
   - 诊断"评委扮演错误"问题

#### 数据库更新
- `debate_messages` 表新增字段：
  - `context_history` (JSON) - 发言时的上下文历史
  - `raw_response` (Text) - 原始响应内容
  - `model_name` (String) - 使用的模型名称

#### 后端改进
- `app/judges/stage_two.py`：
  - 实时记录每次发言的上下文
  - 累积完整的消息历史
  - 保存模型信息

- `app/api/routes.py`：
  - 新增 `/api/config/models` 端点
  - 返回完整的模型配置映射
  - 用于诊断配置问题

#### 前端增强
- `frontend/debug.html`：
  - 每条消息新增"查看调试详情"按钮
  - 展开可查看上下文历史
  - 显示模型名称和原始响应
  - 支持复制原始响应

- `frontend/config.html`：
  - 全新的配置检查页面
  - 展示所有评委的模型映射
  - 诊断指南和常见问题

- `frontend/index.html`：
  - 添加"查看系统配置"按钮

#### 使用场景
- 🔍 **调试模型行为**：查看每次发言时模型看到了什么
- 🐛 **诊断配置错误**：确认每个评委使用的模型是否正确
- 📊 **分析对话流程**：理解评委如何基于历史消息进行回复
- 🎯 **优化提示词**：根据上下文分析改进 system message

---

## [1.2.0] - 2025-11-20

### 🚀 增强功能：阶段二完整调试信息

#### 新增内容
- **阶段二调试上下文**：
  - 每个评委在群聊中的 System Message
  - Selector Prompt（选择器提示词）
  - Initial Message（初始消息汇总）
  - 评委上下文配置（模型、人设等）

#### 数据库更新
- `debate_sessions` 表新增字段：
  - `judge_contexts` - 每个评委的上下文信息
  - `selector_prompt` - 选择器提示词
  - `initial_message` - 初始消息内容

#### 前端改进
- 调试页面阶段二新增：
  - "查看群聊上下文" - 展示初始消息和选择器提示词
  - "查看评委上下文配置" - 展示每个评委的系统消息和模型配置
  - 自动显示评委显示名称

---

## [1.1.0] - 2025-11-20

### ✨ 新增功能：调试详情页面

#### 功能概述
添加了完整的调试系统，允许查看每个 AI 评委在评分过程中的完整请求上下文和回复详情。

#### 主要更新

**1. 数据库扩展** (`app/models/database.py`)
- 在 `JudgeResult` 表中新增字段：
  - `system_message` - 评委收到的系统消息
  - `user_instruction` - 用户指令内容
  - `model_name` - 使用的模型名称
  - `debug_context` - 其他调试上下文信息

**2. 后端改进**
- `app/judges/stage_one.py`：
  - 修改 `build_vision_judges()` 函数，返回调试上下文
  - 在评分过程中保存完整的请求和响应信息
  
- `app/db/crud.py`：
  - 更新 `save_judge_results()` 函数，保存新的调试字段
  
- `app/api/routes.py`：
  - 新增 `GET /api/debug/entry/{entry_id}` 端点
  - 返回作品的完整调试信息（包括阶段一和阶段二）

**3. 前端新增**
- `frontend/debug.html`：
  - 全新的调试详情页面
  - 支持 Tab 切换（阶段一/阶段二）
  - 可折叠的请求上下文和回复详情
  - 复制功能，快速复制原始输出
  - 美观的深色代码显示框
  
- `frontend/index.html`：
  - 在结果页面添加「🔍 查看调试详情」按钮
  - 自动关联到对应作品的调试页面

**4. 路由配置** (`app/main.py`)
- 添加 `/debug.html` 路由，直接返回调试页面

**5. 文档**
- `docs/DEBUG_GUIDE.md`：完整的调试功能使用指南

#### 使用方法

1. 提交作品并完成评分
2. 在结果页面点击「🔍 查看调试详情」
3. 查看每个评委的：
   - 完整的 System Message（评分规范 + 人设）
   - User Instruction（用户指令）
   - 模型名称
   - 原始输出（Raw Output）
   - 解析后的评分结果
4. 切换到「阶段二」查看群聊讨论详情

#### 技术细节

- 所有调试信息自动保存到数据库
- 支持重复提交（会覆盖旧数据）
- 数据库已重新初始化，旧数据不包含调试信息
- 无性能影响，调试信息异步保存

#### 应用场景

- 🎯 优化评委提示词
- 🐛 调试评分问题
- 📈 分析评委行为差异
- 🔧 开发和测试新功能

---

## [1.0.0] - 2025-11-19

### ✨ 初始版本

- 实现阶段一：多评委并发评分
- 实现阶段二：评委群聊讨论
- 前端页面：作品提交和结果展示
- 高级设置：自定义评分规范和评委人设
- 数据库持久化：SQLite
- API 文档：FastAPI 自动生成
